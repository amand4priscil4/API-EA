/**
 * Servi√ßo de IA para Chat
 *
 * Integra com diferentes provedores de IA (OpenAI, Anthropic, Google, etc.)
 */

const axios = require('axios');

class AIService {
  constructor() {
    this.conversations = new Map(); // Armazena hist√≥rico de conversas
    this.provider = process.env.AI_PROVIDER || 'gemini'; // Default Gemini
    this.apiKey = process.env.AI_API_KEY || null;

    // Configura√ß√µes por provedor
    this.configs = {
      openai: {
        baseURL: 'https://api.openai.com/v1',
        model: 'gpt-3.5-turbo',
        endpoint: '/chat/completions'
      },
      anthropic: {
        baseURL: 'https://api.anthropic.com/v1',
        model: 'claude-3-sonnet-20240229',
        endpoint: '/messages'
      },
      gemini: {
        baseURL: 'https://generativelanguage.googleapis.com/v1beta',
        model: 'gemini-pro',
        endpoint: '/models/gemini-pro:generateContent'
      },
      local: {
        baseURL: 'http://localhost:11434/v1', // Ollama local
        model: 'llama2',
        endpoint: '/chat/completions'
      }
    };

    // Sistema de prompts contextuais
    this.systemPrompts = {
      robotics_education: `Voc√™ √© um assistente especializado em rob√≥tica educacional e tecnologia. 
      Seu nome √© "Assistente IA" e voc√™ trabalha junto com o Edu-Ardu para ajudar estudantes.
      
      Caracter√≠sticas:
      - Explique conceitos de forma did√°tica e acess√≠vel
      - Use exemplos pr√°ticos quando poss√≠vel
      - Seja paciente e encorajador
      - Foque em rob√≥tica, Arduino, programa√ß√£o e eletr√¥nica
      - Responda em portugu√™s brasileiro
      - Use emojis ocasionalmente para deixar mais amig√°vel
      - Se n√£o souber algo, seja honesto e sugira recursos
      
      Evite:
      - Respostas muito t√©cnicas sem explica√ß√£o
      - Informa√ß√µes incorretas sobre eletr√¥nica (seguran√ßa √© importante)
      - C√≥digos muito complexos sem explica√ß√£o
      
      Lembre-se: voc√™ est√° ajudando estudantes a aprender rob√≥tica de forma divertida e segura!`,

      general: `Voc√™ √© um assistente IA √∫til e amig√°vel. Responda de forma clara e concisa em portugu√™s brasileiro.`
    };
  }

  /**
   * Envia mensagem para IA e retorna resposta
   */
  async sendMessage(message, sessionId, context = 'general', options = {}) {
    try {
      // Obt√©m ou cria hist√≥rico da conversa
      let conversation = this.conversations.get(sessionId) || [];

      // Adiciona mensagem do usu√°rio ao hist√≥rico
      conversation.push({
        role: 'user',
        content: message,
        timestamp: new Date()
      });

      // Determina qual provedor usar
      const provider = options.provider || this.provider;

      // Se n√£o tem API key, for√ßa usar mock
      const finalProvider = !this.apiKey && provider !== 'mock' ? 'mock' : provider;

      // Gera resposta baseada no provedor
      let response;
      switch (finalProvider) {
        case 'openai':
          response = await this.callOpenAI(conversation, context, options);
          break;
        case 'anthropic':
          response = await this.callAnthropic(conversation, context, options);
          break;
        case 'gemini':
          response = await this.callGemini(conversation, context, options);
          break;
        case 'local':
          response = await this.callLocal(conversation, context, options);
          break;
        case 'mock':
          response = await this.callMock(conversation, context, options);
          break;
        default:
          response = await this.callMock(conversation, context, options);
      }

      // Adiciona resposta da IA ao hist√≥rico
      conversation.push({
        role: 'assistant',
        content: response.text,
        timestamp: new Date(),
        model: response.model,
        provider: finalProvider
      });

      // Limita hist√≥rico a √∫ltimas 20 mensagens para n√£o exceder limite de tokens
      if (conversation.length > 20) {
        conversation = conversation.slice(-20);
      }

      // Salva hist√≥rico atualizado
      this.conversations.set(sessionId, conversation);

      return {
        response: response.text,
        model: response.model,
        provider: finalProvider,
        conversationLength: conversation.length,
        sessionId: sessionId
      };
    } catch (error) {
      console.error('Erro no servi√ßo de IA:', error.message);
      throw new Error(`Falha ao processar mensagem: ${error.message}`);
    }
  }

  /**
   * Chama API da OpenAI
   */
  async callOpenAI(conversation, context, options) {
    const config = this.configs.openai;
    const systemPrompt = this.systemPrompts[context] || this.systemPrompts.general;

    const messages = [
      { role: 'system', content: systemPrompt },
      ...conversation.map(msg => ({
        role: msg.role,
        content: msg.content
      }))
    ];

    const response = await axios.post(
      `${config.baseURL}${config.endpoint}`,
      {
        model: options.model || config.model,
        messages: messages,
        max_tokens: options.maxTokens || 500,
        temperature: options.temperature || 0.7,
        stream: false
      },
      {
        headers: {
          Authorization: `Bearer ${this.apiKey}`,
          'Content-Type': 'application/json'
        }
      }
    );

    return {
      text: response.data.choices[0].message.content,
      model: response.data.model
    };
  }

  /**
   * Chama API da Anthropic (Claude)
   */
  async callAnthropic(conversation, context, options) {
    const config = this.configs.anthropic;
    const systemPrompt = this.systemPrompts[context] || this.systemPrompts.general;

    const messages = conversation.map(msg => ({
      role: msg.role === 'assistant' ? 'assistant' : 'user',
      content: msg.content
    }));

    const response = await axios.post(
      `${config.baseURL}${config.endpoint}`,
      {
        model: options.model || config.model,
        max_tokens: options.maxTokens || 500,
        system: systemPrompt,
        messages: messages
      },
      {
        headers: {
          'x-api-key': this.apiKey,
          'Content-Type': 'application/json',
          'anthropic-version': '2023-06-01'
        }
      }
    );

    return {
      text: response.data.content[0].text,
      model: response.data.model
    };
  }

  /**
   * Chama API do Google Gemini
   */
  async callGemini(conversation, context, options) {
    const config = this.configs.gemini;
    const systemPrompt = this.systemPrompts[context] || this.systemPrompts.general;

    // Formata conversa para Gemini
    const contents = [
      { parts: [{ text: systemPrompt }] },
      ...conversation.map(msg => ({
        parts: [{ text: msg.content }],
        role: msg.role === 'assistant' ? 'model' : 'user'
      }))
    ];

    const response = await axios.post(`${config.baseURL}${config.endpoint}?key=${this.apiKey}`, {
      contents: contents,
      generationConfig: {
        maxOutputTokens: options.maxTokens || 500,
        temperature: options.temperature || 0.7
      }
    });

    return {
      text: response.data.candidates[0].content.parts[0].text,
      model: 'gemini-pro'
    };
  }

  /**
   * Chama modelo local (Ollama)
   */
  async callLocal(conversation, context, options) {
    const config = this.configs.local;
    const systemPrompt = this.systemPrompts[context] || this.systemPrompts.general;

    const messages = [
      { role: 'system', content: systemPrompt },
      ...conversation.map(msg => ({
        role: msg.role,
        content: msg.content
      }))
    ];

    const response = await axios.post(`${config.baseURL}${config.endpoint}`, {
      model: options.model || config.model,
      messages: messages,
      stream: false
    });

    return {
      text: response.data.choices[0].message.content,
      model: response.data.model
    };
  }

  /**
   * Resposta mock para desenvolvimento/teste
   */
  async callMock(conversation, context, options) {
    // Simula delay da API
    await new Promise(resolve => setTimeout(resolve, 1000 + Math.random() * 2000));

    const lastMessage = conversation[conversation.length - 1]?.content || '';

    // Respostas contextuais baseadas na mensagem
    const responses = {
      robotics: [
        '√ìtima pergunta sobre rob√≥tica! ü§ñ Os rob√¥s funcionam atrav√©s da integra√ß√£o de sensores, atuadores e programa√ß√£o. Quer que eu explique algum componente espec√≠fico?',
        'Rob√≥tica √© fascinante! üîß Posso te ajudar com conceitos b√°sicos, programa√ß√£o Arduino, ou projetos pr√°ticos. No que voc√™ est√° mais interessado?',
        'Vamos explorar a rob√≥tica juntos! ‚ö° Desde sensores ultrass√¥nicos at√© motores servo, h√° muito para descobrir. Qual √°rea te desperta mais curiosidade?'
      ],
      arduino: [
        'Arduino √© uma excelente escolha para come√ßar! üíª √â uma plataforma open-source perfeita para aprender eletr√¥nica e programa√ß√£o. Precisa de ajuda com algum projeto espec√≠fico?',
        'Adorei sua pergunta sobre Arduino! üîå Posso te ajudar desde o b√°sico como piscar um LED at√© projetos mais avan√ßados com sensores e motores.',
        'Arduino torna a eletr√¥nica acess√≠vel para todos! üõ†Ô∏è Que tipo de projeto voc√™ tem em mente? Automa√ß√£o residencial, rob√¥, ou algo diferente?'
      ],
      programming: [
        'Programa√ß√£o √© a linguagem dos rob√¥s! üíæ Posso te ajudar com l√≥gica de programa√ß√£o, C++ para Arduino, ou Python para Raspberry Pi. O que voc√™ gostaria de aprender?',
        'C√≥digo √© poder! ‚ö° Desde loops b√°sicos at√© algoritmos complexos, posso te guiar no aprendizado. Em que linguagem voc√™ quer focar?',
        'Programa√ß√£o abre infinitas possibilidades! üöÄ Quer come√ßar com conceitos b√°sicos ou tem algum projeto espec√≠fico em mente?'
      ],
      default: [
        'Interessante! ü§î Como posso te ajudar com isso? Estou aqui para esclarecer d√∫vidas sobre rob√≥tica, programa√ß√£o, eletr√¥nica ou qualquer outro assunto.',
        'Entendi! üí° Posso explicar melhor ou voc√™ tem alguma d√∫vida espec√≠fica? Estou aqui para ajudar no que precisar.',
        'Boa pergunta! üìö Quer que eu detalhe mais algum aspecto espec√≠fico? Sempre posso fornecer exemplos pr√°ticos ou exerc√≠cios.'
      ]
    };

    // Determina categoria da resposta
    let category = 'default';
    if (lastMessage.toLowerCase().includes('robot') || lastMessage.toLowerCase().includes('rob√¥')) {
      category = 'robotics';
    } else if (lastMessage.toLowerCase().includes('arduino')) {
      category = 'arduino';
    } else if (
      lastMessage.toLowerCase().includes('programa') ||
      lastMessage.toLowerCase().includes('c√≥digo')
    ) {
      category = 'programming';
    }

    // Seleciona resposta aleat√≥ria da categoria
    const categoryResponses = responses[category];
    const randomResponse = categoryResponses[Math.floor(Math.random() * categoryResponses.length)];

    return {
      text: randomResponse,
      model: 'Mock AI Assistant'
    };
  }

  /**
   * Obt√©m hist√≥rico de uma conversa
   */
  getConversationHistory(sessionId) {
    return this.conversations.get(sessionId) || [];
  }

  /**
   * Limpa hist√≥rico de uma conversa
   */
  clearConversation(sessionId) {
    this.conversations.delete(sessionId);
    return true;
  }

  /**
   * Lista todas as conversas ativas
   */
  getActiveConversations() {
    const sessions = [];
    for (const [sessionId, conversation] of this.conversations) {
      sessions.push({
        sessionId: sessionId,
        messageCount: conversation.length,
        lastActivity: conversation[conversation.length - 1]?.timestamp || null
      });
    }
    return sessions;
  }

  /**
   * Configura√ß√µes dispon√≠veis
   */
  getAvailableProviders() {
    return Object.keys(this.configs);
  }

  /**
   * Status do servi√ßo
   */
  getStatus() {
    return {
      provider: this.provider,
      hasApiKey: !!this.apiKey,
      activeConversations: this.conversations.size,
      availableProviders: this.getAvailableProviders(),
      systemStatus: 'operational'
    };
  }
}

module.exports = new AIService();
